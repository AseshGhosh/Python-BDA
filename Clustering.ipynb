{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('hack_find').getOrCreate()\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Loads data.\n",
        "dataset = spark.read.csv(\"hack_data.csv\",header=True,inferSchema=True)\n",
        "dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "2GA5LEDi2EfF",
        "outputId": "fd81b9ba-01b9-4819-f1b7-b17390f401ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-12f62d66dfa2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hack_find'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclustering\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Loads data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIG4PDlL2YzR",
        "outputId": "a23fe471-74a5-4cef-d405-2e3d22413388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
            "|summary|Session_Connection_Time| Bytes Transferred|   Kali_Trace_Used|Servers_Corrupted|   Pages_Corrupted|   Location|  WPM_Typing_Speed|\n",
            "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
            "|  count|                    334|               334|               334|              334|               334|        334|               334|\n",
            "|   mean|     30.008982035928145| 607.2452694610777|0.5119760479041916|5.258502994011977|10.838323353293413|       NULL|57.342395209580864|\n",
            "| stddev|     14.088200614636158|286.33593163576757|0.5006065264451406| 2.30190693339697|  3.06352633036022|       NULL| 13.41106336843464|\n",
            "|    min|                    1.0|              10.0|                 0|              1.0|               6.0|Afghanistan|              40.0|\n",
            "|    max|                   60.0|            1330.5|                 1|             10.0|              15.0|   Zimbabwe|              75.0|\n",
            "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVxRwy0W2Z6T",
        "outputId": "b2a79d7d-b974-4087-eec2-26c34c68e89f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A', 'B', 'C', 'D', 'Spoiled']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "feat_cols = ['Session_Connection_Time', 'Bytes Transferred', 'Kali_Trace_Used',\n",
        "             'Servers_Corrupted', 'Pages_Corrupted','WPM_Typing_Speed']\n",
        "vec_assembler = VectorAssembler(inputCols = feat_cols, outputCol='features')\n",
        "final_data = vec_assembler.transform(dataset)\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
        "# Compute summary statistics by fitting the StandardScaler\n",
        "scalerModel = scaler.fit(final_data)"
      ],
      "metadata": {
        "id": "BhXS0uhI2clC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_final_data = scalerModel.transform(final_data)"
      ],
      "metadata": {
        "id": "mPBZGbg82i4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Assuming cluster_final_data is your DataFrame with features\n",
        "\n",
        "# Fit KMeans models\n",
        "kmeans3 = KMeans(featuresCol='scaledFeatures', k=3)\n",
        "kmeans2 = KMeans(featuresCol='scaledFeatures', k=2)\n",
        "model_k3 = kmeans3.fit(cluster_final_data)\n",
        "model_k2 = kmeans2.fit(cluster_final_data)\n",
        "\n",
        "# Make predictions\n",
        "predictions_k3 = model_k3.transform(cluster_final_data)\n",
        "predictions_k2 = model_k2.transform(cluster_final_data)\n",
        "\n",
        "# Evaluate clustering performance using ClusteringEvaluator\n",
        "evaluator = ClusteringEvaluator(featuresCol='scaledFeatures', predictionCol='prediction', metricName='silhouette')\n",
        "\n",
        "# Calculate silhouette score\n",
        "silhouette_k3 = evaluator.evaluate(predictions_k3)\n",
        "silhouette_k2 = evaluator.evaluate(predictions_k2)\n",
        "\n",
        "# Print results\n",
        "print(\"With K=3\")\n",
        "print(\"Silhouette Score = \" + str(silhouette_k3))\n",
        "print('--' * 30)\n",
        "print(\"With K=2\")\n",
        "print(\"Silhouette Score = \" + str(silhouette_k2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75Jxu7K_37L0",
        "outputId": "7dbdda40-5d99-46f7-9ff3-67bcff23cfbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With K=3\n",
            "Silhouette Score = 0.7608455651454915\n",
            "------------------------------------------------------------\n",
            "With K=2\n",
            "Silhouette Score = 0.817646009401248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Assuming cluster_final_data is your DataFrame with features\n",
        "\n",
        "for k in range(2, 9):\n",
        "    # Fit KMeans model\n",
        "    kmeans = KMeans(featuresCol='scaledFeatures', k=k)\n",
        "    model = kmeans.fit(cluster_final_data)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.transform(cluster_final_data)\n",
        "\n",
        "    # Evaluate clustering performance using ClusteringEvaluator\n",
        "    evaluator = ClusteringEvaluator(featuresCol='scaledFeatures', predictionCol='prediction', metricName='silhouette')\n",
        "\n",
        "    # Calculate silhouette score\n",
        "    silhouette = evaluator.evaluate(predictions)\n",
        "\n",
        "    # Print results\n",
        "    print(\"With K={}\".format(k))\n",
        "    print(\"Silhouette Score = \" + str(silhouette))\n",
        "    print('--' * 30)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEKhTzj-2p5P",
        "outputId": "56a31b1f-5f06-4eb5-84bd-223ca2054312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With K=2\n",
            "Silhouette Score = 0.817646009401248\n",
            "------------------------------------------------------------\n",
            "With K=3\n",
            "Silhouette Score = 0.7608455651454915\n",
            "------------------------------------------------------------\n",
            "With K=4\n",
            "Silhouette Score = 0.7195901966635131\n",
            "------------------------------------------------------------\n",
            "With K=5\n",
            "Silhouette Score = 0.6038978744953172\n",
            "------------------------------------------------------------\n",
            "With K=6\n",
            "Silhouette Score = 0.5634378377200768\n",
            "------------------------------------------------------------\n",
            "With K=7\n",
            "Silhouette Score = 0.5611091809595398\n",
            "------------------------------------------------------------\n",
            "With K=8\n",
            "Silhouette Score = 0.48294292896878005\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_k3.transform(cluster_final_data).groupBy('prediction').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qx7BIz2i2sqY",
        "outputId": "dc01c4de-7d76-404b-d28d-e506cd775e62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|prediction|count|\n",
            "+----------+-----+\n",
            "|         1|   88|\n",
            "|         2|   79|\n",
            "|         0|  167|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_k2.transform(cluster_final_data).groupBy('prediction').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6VSFCq6210g",
        "outputId": "19d78ec3-c384-470a-b875-c7c2037856e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|prediction|count|\n",
            "+----------+-----+\n",
            "|         1|  167|\n",
            "|         0|  167|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}